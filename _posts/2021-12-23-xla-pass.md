---
layout: post
title: XLA Pass功能分析
date: 2021-12-23
comments: true
categories: [ "TensorFlow", "XLA" ]
---

- [1. XLA Pass概述](#1-xla-pass概述)
- [2. XLA重点Pass类功能分析](#2-xla重点pass类功能分析)
  - [2.1 GpuConvAlgorithmPicker](#21-gpuconvalgorithmpicker)
  - [2.2 BatchNormExpander](#22-batchnormexpander)
  - [2.3 GpuInstructionFusion](#23-gpuinstructionfusion)
  - [2.4 FusionMerger](#24-fusionmerger)
  - [2.5 GpuMultiOutputFusion](#25-gpumultioutputfusion)
  - [2.6 AlgebraicSimplifier](#26-algebraicsimplifier)
- [3. XLA优化Pass对科学计算模型训练性能影响分析](#3-xla优化pass对科学计算模型训练性能影响分析)
- [4. 附录：`TensorFlow + XLA` VS `Paddle + CINN`](#4-附录tensorflow--xla-vs-paddle--cinn)

> **特别声明：未经授权，禁止转载。** <br>
> 本文对XLA源码分析所使用的TensorFlow代码库commit id为d813f80ded2fe1f8aa4e07ead2cb6fc16451b634（master分支），
> JAX代码库commit id为4dd1f001c626eb15f1a8deac58d97b578a1bd85c(main分支，且使用上述tf对应的commit id)。

## 1. XLA Pass概述

<center>
    <img src="/images/posts/xla/pass_uml.svg" width="100%" alt="XLA Pass UML" title="XLA Pass 类UML图"/>
    <p>图 1. XLA Pass 类UML图</p>
</center>

分析`GpuCompiler::OptimizeHloModule`的调用过程，总结在GPU设备上XLA所使用的Pass类型及其之间的关系如图1所示。由图1可知，GPU上所使用的优化Pass基本是`HloModulePass`和`OpExpanderPass`的子类。去除四个用于保证HLO到LLVM IR之间转换正确性的Pass，在GPU上XLA使用的优化Pass多达75个。

HloPassInterface位于XLA HLO Pass继承关系的最顶层（即所有Pass类的公共基类），它主要定义了*Run*和*RunOnModuleGroup*两个纯虚函数，由其衍生出`HloPassPipeline`、`HloModulePass`和`HloModuleGroupPass`三个子类。XLA HLO Pass优化过程可由多个pipeline构成，每个pipeline即为`HloPassPipeline`的一个对象。GPU上的XLA Pass优化即被分为**spmd-partitioner**[^1]、**spmd-simplify**、**optimization**、**simplification**、**collective-optimizations**、**conv-canonicalization**、**layout-assignment**、**nvptx post-layout-assignment part 1**、**post-layout-assignment**、**nvptx post-layout-assignment part 2**、**fusion**、**horizontal-fusion**、**post-fusion-optimization**以及**GPU-ir-emit-prepare**等14个pipeline。基本上所有的优化Pass均直接或间接继承自`HloModulePass`类。`OpExpanderPass`是`HloModulePass`的一个子类，由其再派生出十数个子类，主要用于各类算子的展开。`HloModuleGroupPass`类在XLA中基本没有被使用到。`HloPassFix`更类似一个修饰类，其模板参数会作为该类的父类，模板参数类被修饰后只会运行固定的迭代次数。

## 2. XLA重点Pass类功能分析
基于TensorFlow 官方ResNet50 benchmark程序[resnet50_graph_test](https://github.com/tensorflow/tensorflow/blob/87462bfac761435a46641ff2f10ad0b6e5414a4b/tensorflow/python/eager/benchmarks/resnet50/resnet50_graph_test.py#L98)，我们在A100-40GB单卡上执行ResNet50的训练过程，以此来分析XLA HLO Pass的优化效果。执行命令中使用了`TF_XLA_FLAGS=--tf_xla_auto_jit=2`来开启XLA加速。
<center>
    <img src="/images/posts/xla/pass_call.svg" width="100%" alt="xla pass call" title="ResNet50模型训练中使用到的Pass调用顺序"/>
    <p>图 2. ResNet50模型训练中使用到的Pass调用顺序<br>（黄色块为pipeline，后跟蓝色块为其内部所应用Passes）</p>
</center>

<center>
    <img src="/images/posts/xla/xla_pass_count.png" width="100%" alt="xla pass count" title="ResNet50模型训练中使用到的Pass及其调用次数"/>
    <p>图 3. ResNet50模型训练中使用到的Pass及其调用次数</p>
</center>

如图2和图3所示，TensorFlow进行ResNet50模型训练并开启XLA时总共触发了72个HLO优化Pass，并且某些Pass的调用次数要大于1，故而Pass调用总次数为97。举例来说，`AlgebraicSimplifier`的调用次数为5，其在`simplification pipeline`、`collective-optimizations pipeline`、`post-fusion optimization pipeline`、`post-layout-assignment pipeline`以及`conv-canonicalization pipeline`等5个pipeline中均被调用一次。图2给出的Pass间连接关系诠释了这些Pass的前后调用依赖。

<center>
    <img src="/images/posts/xla/training_speed.png" width="100%" alt="ResNet50 Training Speed" title="TensorFlow + XLA ResNet50模型 A100 单卡 bs256 训练性能"/>
    <p>图 4. <b>TensorFlow + XLA</b> ResNet50模型 A100 单卡 bs256 训练性能 </p>
</center>

在A100-40GB单卡上执行resnet50_graph_test程序中的`benchmark_graph_train`部分（batch size = 256），得到的性能数据在830 ~ 910 examples/sec之间浮动（如图4所示），这不利于后续对单个Pass所起效果的分析。再三考虑之后，我们选择使用**nsight system**工具获取GPU Kernels的总执行耗时以及GPU Kernels执行加显存操作的总耗时来作为性能指标。图5和图6是我们对72个Pass逐一禁用得到的GPU Kernels总执行耗时和GPU上所有操作总耗时的对比分析柱状图。图5和图6中的横坐标是以毫秒为单位的耗时，纵坐标中每一个标签均以`DIS_`开头，其为`Disable`的缩写。`DIS_None`是基线数据，表示不禁用任何Pass， `DIS_XX`表示禁用某一个Pass。

<center>
    <img src="/images/posts/xla/xla_dis_pass_kernel_time.png" width="100%" alt="xla pass time" title="ResNet50模型训练中使用到的Pass效果分析（batch size=256时的GPU Kernels总执行耗时）"/>
    <p>图 5. ResNet50模型训练中使用到的Pass效果分析（batch size=256时的GPU Kernels总执行耗时）</p>
</center>

<center>
    <img src="/images/posts/xla/xla_dis_pass_total_time.png" width="100%" alt="xla pass time" title="ResNet50模型训练中使用到的Pass效果分析（batch size=256时的GPU操作总耗时）"/>
    <p>图 6. ResNet50模型训练中使用到的Pass效果分析（batch size=256时的GPU操作总耗时）</p>
</center>

*注意：因禁用`GpuConvPaddingLegalization`、`GpuLayoutAssignment`和`ReductionDimensionGrouper`三个Pass中的任意一个均会导致程序运行出现coredump等严重错误，因此无法测量出禁用这三个Pass时的性能数据。*

<center>
    <img src="/images/posts/xla/xla_dis_pass_kernel_speedup.png" width="100%" alt="xla pass speedup" title="ResNet50模型训练中使用到的Pass效果分析（高亮关闭后GPU Kernels性能下降1%的Pass）"/>
    <p>图 7. ResNet50模型训练中使用到的Pass效果分析（高亮关闭后GPU Kernels性能下降1%的Pass）</p>
</center>

<center>
    <img src="/images/posts/xla/xla_dis_pass_total_speedup.png" width="100%" alt="xla pass speedup" title="ResNet50模型训练中使用到的Pass效果分析（高亮关闭后GPU操作总性能下降1%的Pass）"/>
    <p>图 8. ResNet50模型训练中使用到的Pass效果分析（高亮关闭后GPU操作总性能下降1%的Pass）</p>
</center>

对图5和图6中的数据分别做归一化处理后（除以基线数据耗时），可得到图7和图8。图7和图8中对禁用后性能下降超过1%的Pass做了红色高亮处理。分析图7中的红色高亮部分，可得禁用**BatchNormExpander**、**FusionMerger**、**GpuConvAlgorithmPicker**、**GpuInstructionFusion**、**GpuMultiOutputFusion**等5个Pass后，GPU Kernel总执行耗时增加比较明显。分析图8中的红色高亮部分，可得禁用**BatchNormExpander**、**FusionMerger**、**GpuConvAlgorithmPicker**、**GpuInstructionFusion**、**GpuMultiOutputFusion**、**SortSimplifier**等6个Pass后，GPU Kernels执行加显存操作的总耗时增加比较明显。

* **GPU Kernels总执行耗时影响程度(性能下降比例)**：<br> GpuConvAlgorithmPicker(57%) > GpuInstructionFusion(50%) > BatchNormExpander(20%) > FusionMerger(10%) > GpuMultiOutputFusion(7%)
* **GPU Kernels执行加显存操作的总耗时影响程度(性能下降比例)**: GpuConvAlgorithmPicker(52%) > GpuInstructionFusion(47%) > BatchNormExpander(17%) > FusionMerger (6%) > GpuMultiOutputFusion(2%)  > SortSimplifier(1%)

因为GPU Kernels执行和显存操作(D2H/H2D/D2D/memset)可能存在并行发生的情况，所以GPU Kernels执行加显存操作的总耗时并不能正确表示训练性能。因此，这里我们主要还是以GPU Kernels总执行耗时作为性能对比指标。根据上述各个XLA HLO Pass对ResNet50模型训练性能的影响程度，下面我们重点分析GpuConvAlgorithmPicker、GpuInstructionFusion、BatchNormExpander、FusionMerger和GpuMultiOutputFusion、AlgebraicSimplifier等六个Pass。

### 2.1 GpuConvAlgorithmPicker

<center>
    <img src="/images/posts/xla/gpu_conv_algorithm_picker.svg" width="100%" alt="GpuConvAlgorithmPicker" title="GpuConvAlgorithmPicker相关类UML图"/>
    <p>图 9. GpuConvAlgorithmPicker相关类UML图</p>
</center>

`GpuConvAlgorithmPicker`类主要用于将HLO的CustomCalls修改为cudnn卷积，并为每一个卷积操作选择最佳算法以及为CustomCalls添加显式的暂存空间(scratch space[^2]，即workspace)，其定义如图9所示。`Run`方法是该类的功能入口，其作用对象是HloModule。它基于后序遍历模式对HloModule的每个非融合类型Computation调用私有方法`RunOnComputation`以处理选定的HloComputation。RunOnComputation方法会遍历其内部的所有Instructions，并对每一条CustomCall卷积指令调用`RunOnInstruction`方法进行处理。RunOnInstruction方法会调用`PickBestAlgorithm`方法以便为指定的CustomCall卷积指令选择最佳算法。

[xla.proto](https://github.com/tensorflow/tensorflow/blob/76bc1a8a691eb6d21dac3cd54a8c3ae984855f2c/tensorflow/compiler/xla/xla.proto#L183)中定义了`message DebugOptions`，它是XLA的调试选项，可在任何时刻被改变，但不保证前向和反向中的兼容性。DebugOptions中存在两个与`GpuConvAlgorithmPicker`类相关的属性，即`xla_gpu_autotune_level`和`xla_gpu_strict_conv_algorithm_picker`，它们的定义如下：

```protobuf
message DebugOptions {
  ...
  // 0:   Disable gemm and convolution autotuning.
  // 1:   Enable autotuning, but disable correctness checking.
  // 2:   Also set input/output buffers to random numbers during autotuning.
  // 3:   Also reset input/output buffers to random numbers after autotuning each
  //      algorithm.
  // 4+:  Also check for correct outputs and for out-of-bounds reads/writes.
  //
  // Default: 4.
  int32 xla_gpu_autotune_level = 123;
  
  // If true, abort immediately when conv algorithm picker fails, rather than
  // logging a warning and proceeding with fallback. Default: True.
  bool xla_gpu_strict_conv_algorithm_picker = 156;

  // Crashes the program when any kind of verification fails, instead of just
  // logging the failures. One example is cross checking of convolution results
  // among different algorithms. Default: False.
  bool xla_gpu_crash_on_verification_failures = 101;

  // Denylist for cuDNN convolutions.
  // constexpr char kDefaultDenylist[] = R"pb(
  //   entries {
  //       hlo: "(f32[4,32,32,32]{2,1,3,0}, u8[0]{0}) custom-call(f32[4,32,32,32]{2,1,3,0}, f32[5,5,32,32]{1,0,2,3}), window={size=5x5 // pad=2_2x2_2}, dim_labels=b01f_01io->b01f, custom_call_target=\"__cudnn$convForward\", backend_config=\"{conv_result_scale:1}\""
  //       cc { major: 7 }
  //       cudnn_version { major: 7 minor: 6 patch: 4 }
  //       algos { id: 7 }
  //       blas_version: "10201"
  //   }
  //   entries {
  //       hlo: "(f32[4,32,32,32]{2,1,3,0}, u8[0]{0}) custom-call(f32[4,32,32,32]{2,1,3,0}, f32[5,5,32,32]{1,0,2,3}), window={size=5x5 // pad=2_2x2_2}, dim_labels=b01f_01io->b01f, custom_call_target=\"__cudnn$convForward\", backend_config=\"{conv_result_scale:1}\""
  //       cc { major: 7 }
  //       cudnn_version { major: 7 minor: 6 patch: 4 }
  //       algos { id: 7 tensor_ops: true }
  //       blas_version: "10201"
  //   }
  //   entries {
  //       hlo: "(f16[3,3,256,256]{2,1,0,3}, u8[0]{0}) custom-call(f16[2048,7,7,256]{3,2,1,0}, f16[2048,7,7,256]{3,2,1,0}), window=// {size=3x3 pad=1_1x1_1}, dim_labels=b01f_01io->b01f, custom_call_target=\"__cudnn$convBackwardFilter\", backend_config=\"// {\\\"algorithm\\\":\\\"0\\\",\\\"tensor_ops_enabled\\\":false,\\\"conv_result_scale\\\":1,\\\"activation_mode\\\":\\\"0\\\",// \\\"side_input_scale\\\":0}\""
  //       cc { major: 7 }
  //       cudnn_version { major: 8 minor: 2 patch: 1 } algos
  //       [ { id: 0 tensor_ops: true }
  //       , { id: 0 }]
  //       blas_version: "11402"
  //   }
  // )pb";
  // message AlgorithmDenylist的定义详见：https://github.com/tensorflow/tensorflow/blob/18ebe824d2f6f20b09839cb0a0073032a2d6c5fe/tensorflow/compiler/xla/service/gpu/gpu_autotuning.proto#L31
  string xla_gpu_algorithm_denylist_path = 128;

  ...
}
```

* `xla_gpu_autotune_level`控制着gemm和convolution的autotuning过程。其默认值是4，意味着开启gemm和convolution的autotuning过程，并且会检查每次tuning的输出结果正确性以及读写越界与否。设置xla_gpu_autotune_level为0，则关闭gemm和convolution的autotuning过程。可使用`XLA_FLAGS=--xla_gpu_autotune_level=0~4`设置其值。
* 当`xla_gpu_strict_conv_algorithm_picker`设置为True时，在卷积算法选择失败时会立即终止程序而不是仅仅打印一条警告信息并回退继续执行。其默认值为True。可使用`XLA_FLAGS=--xla_gpu_strict_conv_algorithm_picker=true/false`设置其值。
* XLA_FLAGS同时设置多个flag的值的方法：`XLA_FLAGS="--xla_gpu_strict_conv_algorithm_picker=false --xla_gpu_autotune_level=0"`。

卷积算法的选择过程通过`GpuConvAlgorithmPicker::PickBestAlgorithm`方法完成，该方法存在cache机制，即如果给定指令的卷积算法tune结果(AutotuneResult类型)已经存在，则直接返回tune结果。`AutotuneResult`定义在[autotuning.proto](https://github.com/tensorflow/tensorflow/blob/713f19273c21b973e00ade908e345b8710d4aa33/tensorflow/core/protobuf/autotuning.proto#L27)中，其具体定义如下：
```protobuf
message AutotuneResult {
  enum FailureKind {
    UNKNOWN = 0;

    // Algorithm wrote memory outside its output buffers.
    REDZONE_MODIFIED = 1;

    // Algorithm gave a different result from a reference algorithm.
    WRONG_RESULT = 2;

    // Algorithm was rejected for failing to run or for known bugs.
    DISQUALIFIED = 3;
  }

  message FailureResult {
    FailureKind kind = 1;
    string msg = 2;

    // For failure_kind == WRONG_RESULT, this field indicates the reference
    // configuration that we compared against.
    //
    // Note that the reference algorithm isn't always correct.  However,
    // empirically it's more correct, as it's "algo 0", less fancy than the
    // compared one.
    oneof key {
      ConvKey reference_conv = 11;
      GemmKey reference_gemm = 12;
      CudaConvPlanKey reference_cuda_conv_plan = 14;
      stream_executor.dnn.AlgorithmProto reference_algorithm = 15;
    }

    int64 buffer_address = 13;
  }

  // Legacy and unused in new data; superseded by AlgorithmProto.
  message ConvKey {
    int64 algorithm = 1;
    bool tensor_ops_enabled = 2;
  }

  message GemmKey {
    int64 algorithm = 1;
  }

  // Legacy and unused in new data; superseded by AlgorithmProto.
  message CudaConvPlanKey {
    string exec_plan_id = 1;
  }

  int64 scratch_bytes = 8;
  google.protobuf.Duration run_time = 9;

  FailureResult failure = 7;

  oneof key {
    ConvKey conv = 5;
    GemmKey gemm = 6;
    CudaConvPlanKey cuda_conv_plan = 15;
    stream_executor.dnn.AlgorithmProto algorithm = 16;
  }

  // Next ID: 17
}
```

`PickBestAlgorithm`方法中的cache未命中时将根据平台类型调用`PickBestAlgorithmNoCacheRocm`或者`PickBestAlgorithmNoCacheCuda`方法，下面我们重点关注`PickBestAlgorithmNoCacheCuda`方法。


### 2.2 BatchNormExpander
<center>
    <img src="/images/posts/xla/batch_norm_expander.svg" width="100%" alt="BatchNormExpander" title="BatchNormExpander相关类UML图"/>
    <p>图 10. BatchNormExpander相关类UML图</p>
</center>

`BatchNormExpander`用于将batch norm操作改写为多个更细粒度的操作。将大Op打散为更细粒度的小Op有助于后续的通用融合逻辑。

### 2.3 GpuInstructionFusion

<center>
    <img src="/images/posts/xla/gpu_instruction_fusion.svg" width="80%" alt="GpuInstructionFusion" title="GpuInstructionFusion相关类UML图"/>
    <p>图 11. GpuInstructionFusion相关类UML图</p>
</center>

`GpuInstructionFusion`用于融合HLO指令。传统意义上，指令融合操作是在垂直方向上进行的，这意味将生产者指令融合到其消费者指令中，这样在生成代码时计算它们值的循环将被融合在一起。其重写定义的`ShouldFuse`方法用于选择可被融合的指令类型。

### 2.4 FusionMerger

<center>
    <img src="/images/posts/xla/fusion_merger.svg" width="70%" alt="FusionMerger" title="FusionMerger相关类UML图"/>
    <p>图 12. FusionMerger相关类UML图</p>
</center>

`FusionMerger`尝试合并融合指令以降低内存带宽占用及减少kernel launch开销。

### 2.5 GpuMultiOutputFusion

<center>
    <img src="/images/posts/xla/gpu_multi_output_fusion.svg" width="80%" alt="GpuMultiOutputFusion" title="GpuMultiOutputFusion相关类UML图"/>
    <p>图 13. GpuMultiOutputFusion相关类UML图</p>
</center>

`GpuMultiOutputFusion`用于GPU后端的同层次兄弟指令以及生产者-消费者指令的多输出融合，以降低内存带宽占用。


### 2.6 AlgebraicSimplifier

AlgebraicSimplifierl类在若干个优化pipeline中均有使用，它主要提了一些代数化简功能。
<center>
    <img src="/images/posts/xla/algebraic_simplifier.svg" width="100%" alt="AlgebraicSimplifier" title="AlgebraicSimplifier相关类UML图"/>
    <p>图 14. AlgebraicSimplifier相关类UML图</p>
</center>

如图14所示，构造`AlgebraicSimplifier`对象时需要传入一个`AlgebraicSimplifierOptions`对象，其定义了一系列的代数化简选项，如与`layout`、`dot`、`conv`、`scalar multiply reduction`、`padding`及`transpose`相关选项。`AlgebraicSimplifier`类重写了父类的`Run`方法，运行`Run`方法时会自动创建一个`AlgebraicSimplifierVisitor`对象*visitor*，之后使用*visitor*对`HloModule`中的每个`HloComputation`进行处理。每个`HloComputation`对象会使用传入的`AlgebraicSimplifierVisitor`对象从`Root HloInstruction`开始使用后序DFS遍历方式处理该computation中的每一条指令。`AlgebraicSimplifierVisitor`对象根据每条`HloInstruction`对象的`HloOpcode`值调用相应的`HandleXXX`方法处理该指令。

`AlgebraicSimplifierVisitor`类会对`Abs`、`Add`、`And`、`Bitcast`、`BitcastConvert`、`Broadcast`、`Compare`、`Concatenate`、`Constant`、`Copy`、`Convert`、`Complex`、`Real`、`Imag`、`Iota`、`Convolution`、`Divide`、`Dot`、`Gather`、`GetTupleElement`、`Log`、`Maximum`、`Minimum`、`Clamp`、`Multiply`、`Negate`、`Not`、`Or`、`Pad`、`Power`、`Remainder`、`Reshape`、`Reduce`、`ReduceWindow`、`Reverse`、`Rsqrt`、`Slice`、`Sqrt`、`DynamicSlice`、`DynamicUpdateSlice`、`Scatter`、`Select`、`Sort`、`Transpose`、`Subtract`和`Map`等46个算子进行代数化简处理。

* `HandleAbs`：当`Abs`的操作数为正时，进行`Abs(A) => A`的简化。
* `HandleAdd`: 对`Add`进行的简化操作如下：
  - `A + 0 => A`
  - `0 + A => A`
  - 将常数放在加法的右边，方便后续重分配规则的简化，即`Const + A => A + Const`
  - 对加法的操作数进行重新分配，方便后续的常量折叠，即`(A + C1) + C2 => A + (C1 + C2)`


## 3. XLA优化Pass对科学计算模型训练性能影响分析

接下来，我们使用科学计算模型（Laplace）分析XLA各个Pass的优化效果，该模型的代码实现详见[此处](https://github.com/levi131/jax/blob/main/examples/laplace.py)。其中，我们将Dennse层输出节点数512设置为`50`。使用A100-40GB单卡训练Laplace模型，batch size设为101 * 101（*注意：这里一个epoch仅包括一个batch*）。在跳过第一个epoch后，累积运行2000个epoch，并通过python的`time.time()`语句获取总执行耗时以及通过nsight system工具获取GPU Kernels执行耗时。因为分别关闭AlgebraicSimplifier、CallInliner、DotDecomposer、GpuLayoutAssignment、ReductionDimensionGrouper、ReductionLayoutNormalizer或者TransposeFolding等7个Pass之一时均会导致程序执行错误，因此只统计到65个Pass对Laplace模型的训练性能影响程度。

> 为了确保测量准确性，总执行耗时和GPU Kernels执行耗时相关的实验被重复进行了三次。因为每次得到的结果均类似，所以认为性能实验测量结果是准确的。下文的数据分析是针对其中某一次实验结果进行的。

<center>
    <img src="/images/posts/xla/jax_gpukernel.png" width="100%" alt="xla pass time" title="Laplace模型使用到的Pass效果分析（batch size=101 * 101时的GPU Kernels运行时间）"/>
    <p>图 15. Laplace模型使用到的Pass效果分析（batch size=101 * 101时的GPU Kernels运行时间）</p>
</center>

<center>
    <img src="/images/posts/xla/jax_overall.png" width="100%" alt="xla pass time" title="Laplace模型使用到的Pass效果分析（batch size=101 * 101时的总执行耗时）"/>
    <p>图 16. Laplace模型使用到的Pass效果分析（batch size=101 * 101时的总执行耗时）</p>
</center>

图15和图16分别给出了关闭任意一个Pass时Laplace模型GPU Kernels平均总运行时间对比和平均总执行耗时对比。分析图15和图16，可以发现基线（所有Pass全开）总耗时要比基线GPU Kernels总执行时间小。这是因为nsight system虽然可获取GPU Kernels执行信息，但其也会导致总执行耗时增加。因此，GPU Kernels运行时间的测量和总执行耗时的测量是通过两次实验获取的，这难免会导致一些测量误差。

<center>
    <img src="/images/posts/xla/jax_gpukernel_rel.png" width="100%" alt="xla pass speedup" title="Laplace模型使用到的Pass效果分析（红色高亮关闭后GPU Kernels性能下降5%以上的Pass）"/>
    <p>图 17. Laplace模型使用到的Pass效果分析（红色高亮关闭后GPU Kernels性能下降5%以上的Pass）</p>
</center>

<center>
    <img src="/images/posts/xla/jax_overall_rel.png" width="100%" alt="xla pass speedup" title="Laplace模型使用到的Pass效果分析（红色高亮关闭后整体性能下降5%以上的Pass）"/>
    <p>图 18. Laplace模型使用到的Pass效果分析（红色高亮关闭后整体性能下降5%以上的Pass）</p>
</center>

我们分别使用基线数据对图15和图16进行归一化处理后（baseline / each），得到图17和图18。图17和图18均红色高亮出了对性能影响较大的Pass。
* 分析图17可知，关闭后导致GPU Kernels性能下降5%以上的Pass包括(括号内为关闭后性能下降比例)：<br>
  GemmRewriter(95.8%) > GpuInstructionFusion(48.9%) > ScatterExpander(20.8%) > GpuTreeReductionRewriter(19.4%) > FusionMerger(14.9%) > GpuMultiOutputFusion(7.9%)
* 分析图18可知，关闭后导致整体性能下降5%以上的Pass包括(括号内为关闭后性能下降比例)：<br>
  GemmRewriter(96.0%) > GpuInstructionFusion(47.6%) > ScatterExpander(25.0%) > GpuTreeReductionRewriter(21.5%) > FusionMerger(16.9%) > DotMerger(15.5%) > GpuMultiOutputFusion(15.0%) > HloCSE(8.8%) > ReductionDegenerateDimRemover(8.2%) > SortSimplifier(6.8%) > RngBitGeneratorExpander(6.1%)

对比图15和图16，可以发现一个有趣的现象，关闭`DotMerger` Pass后，GPU Kernels的执行时间减少了，但是整体耗时却增加了。这说明DotMerger会带来recompute计算，但是却可以减少IO操作，因此可以带来性能的提升。但是三次实验中也存在一次实验关闭`DotMerger`后，总执行性能提高了10%左右。计算的增加和IO开销的减少之间是一种折中或者博弈：如果计算的增加带来的性能下降比IO数减少带来的性能提升要高，那么总耗时必然会增加。

虽然图18显示的`GpuHorizontalInputFusion`关闭后对整体性能影响不大，但三次实验中存在一次关闭`GpuHorizontalInputFusion`后，整体性能下降了11%左右。因此该Pass可能也需要多关注一些。

因为使用python的`time.time()`测量总体耗时可能存在一些误差，因此我们在此优先关注整体性能影响在10%以上的Pass。结合对GPU Kernels性能影响显著的Pass，可得出在Laplace模型上需要重点关注如下8个Pass的功能：
* GemmRewriter
* GpuInstructionFusion
* GpuMultiOutputFusion
* GpuHorizontalInputFusion
* FusionMerger
* ScatterExpander
* GpuTreeReductionRewriter
* DotMerger



## 4. 附录：`TensorFlow + XLA` VS `Paddle + CINN`

<center> 表 1. TensorFlow + XLA 和 Paddle + CINN在ResNet50模型上的训练性能对比<br>实验配置：A100-40GB单卡，batch size = 256</center>

| Framework        | GPU Kernel Time<br>(ms) | GPU Kernel+Mem Time<br>(ms) | Speed<br>(images/sec) |
|------------------|---------------------|-------------------------|--------------------|
| TensorFlow + XLA | 257                 | 290~310                 | 830 ~ 910          |
| Paddle + CINN    | 296                 | 310                     | 862                |


[^1]: SPMD (Single-Program-Multiple-Data) 是最常用的分布式模式，即数据并行。
[^2]: In general, a scratch space is a temporary location in memory that allows for something to be saved.